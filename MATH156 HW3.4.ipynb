{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07b5872-7e38-4ad7-b8f5-011031c8036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: [209 132]\n",
      "Validation set size: [72 42]\n",
      "Iteration 1, Loss: 94.90200380175136\n",
      "Iteration 2, Loss: 77.32785974747729\n",
      "Iteration 3, Loss: 65.5783965386893\n",
      "Iteration 4, Loss: 60.93627806830611\n",
      "Iteration 5, Loss: 57.659475487232065\n",
      "Iteration 6, Loss: 54.851760401550955\n",
      "Iteration 7, Loss: 52.88330287785462\n",
      "Iteration 8, Loss: 51.42916826428455\n",
      "Iteration 9, Loss: 50.112749186686194\n",
      "Iteration 10, Loss: 47.47409953340821\n",
      "Iteration 11, Loss: 46.06411081628019\n",
      "Iteration 12, Loss: 45.08810669911742\n",
      "Iteration 13, Loss: 44.02591287183318\n",
      "Iteration 14, Loss: 43.39085205786545\n",
      "Iteration 15, Loss: 42.66563099660583\n",
      "Iteration 16, Loss: 42.01319003977446\n",
      "Iteration 17, Loss: 40.955853830164926\n",
      "Iteration 18, Loss: 40.369044736699465\n",
      "Iteration 19, Loss: 39.43513047258607\n",
      "Iteration 20, Loss: 39.234539328981455\n",
      "Iteration 21, Loss: 38.54737990836619\n",
      "Iteration 22, Loss: 37.87919335089874\n",
      "Iteration 23, Loss: 37.50967923296462\n",
      "Iteration 24, Loss: 37.016953322585216\n",
      "Iteration 25, Loss: 36.599587967210866\n",
      "Iteration 26, Loss: 36.24945606797911\n",
      "Iteration 27, Loss: 36.19624224993112\n",
      "Iteration 28, Loss: 35.785867506138466\n",
      "Iteration 29, Loss: 35.564311312638125\n",
      "Iteration 30, Loss: 35.29431534689235\n",
      "Iteration 31, Loss: 34.90260944919095\n",
      "Iteration 32, Loss: 34.46312844877213\n",
      "Iteration 33, Loss: 34.18264480862884\n",
      "Iteration 34, Loss: 33.69921266877919\n",
      "Iteration 35, Loss: 33.46645875732928\n",
      "Iteration 36, Loss: 33.463266669950556\n",
      "Iteration 37, Loss: 33.23926985072515\n",
      "Iteration 38, Loss: 33.0112016215714\n",
      "Iteration 39, Loss: 32.671962458665966\n",
      "Iteration 40, Loss: 32.42740167260504\n",
      "Iteration 41, Loss: 32.29955319267526\n",
      "Iteration 42, Loss: 32.06221949531851\n",
      "Iteration 43, Loss: 31.869901216584\n",
      "Iteration 44, Loss: 31.93354626810778\n",
      "Iteration 45, Loss: 32.09421452320689\n",
      "Iteration 46, Loss: 32.12078451321749\n",
      "Iteration 47, Loss: 31.809145228522365\n",
      "Iteration 48, Loss: 31.791526108196507\n",
      "Iteration 49, Loss: 31.23971546910881\n",
      "Iteration 50, Loss: 31.253010364117685\n",
      "Iteration 51, Loss: 30.86428595496865\n",
      "Iteration 52, Loss: 30.75901384299292\n",
      "Iteration 53, Loss: 30.615814951779964\n",
      "Iteration 54, Loss: 30.581092648348022\n",
      "Iteration 55, Loss: 30.20392443659243\n",
      "Iteration 56, Loss: 29.996296171038086\n",
      "Iteration 57, Loss: 29.84211339132694\n",
      "Iteration 58, Loss: 30.090211182863786\n",
      "Iteration 59, Loss: 30.022883960733594\n",
      "Iteration 60, Loss: 29.96473305915322\n",
      "Iteration 61, Loss: 29.89891709849214\n",
      "Iteration 62, Loss: 29.541800800602935\n",
      "Iteration 63, Loss: 29.496428669680654\n",
      "Iteration 64, Loss: 29.37386990912429\n",
      "Iteration 65, Loss: 29.196892345700824\n",
      "Iteration 66, Loss: 29.242950328263333\n",
      "Iteration 67, Loss: 29.05131571875988\n",
      "Iteration 68, Loss: 28.984938726416882\n",
      "Iteration 69, Loss: 28.906911315803473\n",
      "Iteration 70, Loss: 28.91209415447414\n",
      "Iteration 71, Loss: 28.790838742449303\n",
      "Iteration 72, Loss: 28.600872326626828\n",
      "Iteration 73, Loss: 28.456724775037934\n",
      "Iteration 74, Loss: 28.233003890174924\n",
      "Iteration 75, Loss: 28.270535881834284\n",
      "Iteration 76, Loss: 28.18764231438512\n",
      "Iteration 77, Loss: 27.983356559599105\n",
      "Iteration 78, Loss: 27.998429222381418\n",
      "Iteration 79, Loss: 27.89683734052484\n",
      "Iteration 80, Loss: 27.80616133640726\n",
      "Iteration 81, Loss: 27.764976547052417\n",
      "Iteration 82, Loss: 27.774237271140493\n",
      "Iteration 83, Loss: 27.73884708031283\n",
      "Iteration 84, Loss: 27.74380105525743\n",
      "Iteration 85, Loss: 27.551442341149702\n",
      "Iteration 86, Loss: 27.493210230513718\n",
      "Iteration 87, Loss: 27.51508767178491\n",
      "Iteration 88, Loss: 27.51766858925435\n",
      "Iteration 89, Loss: 27.506613482569506\n",
      "Iteration 90, Loss: 27.563758081592866\n",
      "Iteration 91, Loss: 27.37457909450911\n",
      "Iteration 92, Loss: 27.23117616372874\n",
      "Iteration 93, Loss: 27.02287852668526\n",
      "Iteration 94, Loss: 27.00977539947757\n",
      "Iteration 95, Loss: 26.963280835259603\n",
      "Iteration 96, Loss: 26.894451790739154\n",
      "Iteration 97, Loss: 26.93259079158945\n",
      "Iteration 98, Loss: 26.845050718417355\n",
      "Iteration 99, Loss: 26.69605883960738\n",
      "Iteration 100, Loss: 26.56322580022411\n",
      "Iteration 101, Loss: 26.718450141871372\n",
      "Iteration 102, Loss: 26.814883032123305\n",
      "Iteration 103, Loss: 26.825421568811702\n",
      "Iteration 104, Loss: 26.625053712118785\n",
      "Iteration 105, Loss: 26.582082807374096\n",
      "Iteration 106, Loss: 26.523434332833958\n",
      "Iteration 107, Loss: 26.317804069540767\n",
      "Iteration 108, Loss: 26.216575782568615\n",
      "Iteration 109, Loss: 26.072614542839922\n",
      "Iteration 110, Loss: 26.052873904681366\n",
      "Iteration 111, Loss: 25.945647423000224\n",
      "Iteration 112, Loss: 25.894858326663467\n",
      "Iteration 113, Loss: 25.910067830694416\n",
      "Iteration 114, Loss: 25.832739812417213\n",
      "Iteration 115, Loss: 25.76676342218909\n",
      "Iteration 116, Loss: 25.720075864171914\n",
      "Iteration 117, Loss: 25.671273470668787\n",
      "Iteration 118, Loss: 25.679137438272946\n",
      "Iteration 119, Loss: 25.753193281013143\n",
      "Iteration 120, Loss: 25.771846979748755\n",
      "Iteration 121, Loss: 25.6483602547553\n",
      "Iteration 122, Loss: 25.559099259848885\n",
      "Iteration 123, Loss: 25.504164249131513\n",
      "Iteration 124, Loss: 25.442790787745167\n",
      "Iteration 125, Loss: 25.392391752993756\n",
      "Iteration 126, Loss: 25.350015449180272\n",
      "Iteration 127, Loss: 25.30582923057723\n",
      "Iteration 128, Loss: 25.242705461051393\n",
      "Iteration 129, Loss: 25.268615223592438\n",
      "Iteration 130, Loss: 25.269358160649617\n",
      "Iteration 131, Loss: 25.185241747273366\n",
      "Iteration 132, Loss: 25.125402871620345\n",
      "Iteration 133, Loss: 25.073045728027388\n",
      "Iteration 134, Loss: 25.022766522330162\n",
      "Iteration 135, Loss: 24.975028348852753\n",
      "Iteration 136, Loss: 24.918014489707048\n",
      "Iteration 137, Loss: 24.860554675841733\n",
      "Iteration 138, Loss: 24.83416247558322\n",
      "Iteration 139, Loss: 24.791143295542827\n",
      "Iteration 140, Loss: 24.77574154047183\n",
      "Iteration 141, Loss: 24.737431380313055\n",
      "Iteration 142, Loss: 24.72325676936451\n",
      "Iteration 143, Loss: 24.6679186229671\n",
      "Iteration 144, Loss: 24.62788576864208\n",
      "Iteration 145, Loss: 24.600074700338745\n",
      "Iteration 146, Loss: 24.564704645590005\n",
      "Iteration 147, Loss: 24.553928371360414\n",
      "Iteration 148, Loss: 24.55512160244701\n",
      "Iteration 149, Loss: 24.508846683360915\n",
      "Iteration 150, Loss: 24.473981716339576\n",
      "Iteration 151, Loss: 24.44633067025489\n",
      "Iteration 152, Loss: 24.418366299404624\n",
      "Iteration 153, Loss: 24.46624275013489\n",
      "Iteration 154, Loss: 24.404250220597646\n",
      "Iteration 155, Loss: 24.291873907741945\n",
      "Iteration 156, Loss: 24.221939897218753\n",
      "Iteration 157, Loss: 24.33054275645232\n",
      "Iteration 158, Loss: 24.25815637337398\n",
      "Iteration 159, Loss: 24.189711748605603\n",
      "Iteration 160, Loss: 24.120345072188073\n",
      "Iteration 161, Loss: 24.11202743307779\n",
      "Iteration 162, Loss: 24.063939113320885\n",
      "Iteration 163, Loss: 24.08429070759805\n",
      "Iteration 164, Loss: 24.067970853695368\n",
      "Iteration 165, Loss: 24.038823065651112\n",
      "Iteration 166, Loss: 24.013183335544536\n",
      "Iteration 167, Loss: 23.988639422048344\n",
      "Iteration 168, Loss: 23.965873640430996\n",
      "Iteration 169, Loss: 23.941346173944773\n",
      "Iteration 170, Loss: 23.902417220321116\n",
      "Iteration 171, Loss: 23.851812990777553\n",
      "Iteration 172, Loss: 23.83068787427036\n",
      "Iteration 173, Loss: 23.78716445990087\n",
      "Iteration 174, Loss: 23.814180165737845\n",
      "Iteration 175, Loss: 23.837656993428528\n",
      "Iteration 176, Loss: 23.759965687838566\n",
      "Iteration 177, Loss: 23.753516860453278\n",
      "Iteration 178, Loss: 23.70425029973986\n",
      "Iteration 179, Loss: 23.650277016875165\n",
      "Iteration 180, Loss: 23.657241507353454\n",
      "Iteration 181, Loss: 23.6057110530248\n",
      "Iteration 182, Loss: 23.58790092653765\n",
      "Iteration 183, Loss: 23.580301943742615\n",
      "Iteration 184, Loss: 23.593650478595922\n",
      "Iteration 185, Loss: 23.548199777637357\n",
      "Iteration 186, Loss: 23.54137467668047\n",
      "Iteration 187, Loss: 23.52523311226392\n",
      "Iteration 188, Loss: 23.53525002511276\n",
      "Iteration 189, Loss: 23.589040811813746\n",
      "Iteration 190, Loss: 23.573755096568558\n",
      "Iteration 191, Loss: 23.50174187758168\n",
      "Iteration 192, Loss: 23.40631933087205\n",
      "Iteration 193, Loss: 23.42944203989382\n",
      "Iteration 194, Loss: 23.484203469164402\n",
      "Iteration 195, Loss: 23.39361574703215\n",
      "Iteration 196, Loss: 23.358074578001\n",
      "Iteration 197, Loss: 23.296725619120778\n",
      "Iteration 198, Loss: 23.255946367373475\n",
      "Iteration 199, Loss: 23.258420235170576\n",
      "Iteration 200, Loss: 23.233332414094335\n",
      "Test Accuracy: 0.9824561403508771\n",
      "Precision: 0.95\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9743589743589743\n",
      "Summary of findings will be provided based on the above metrics and any observations during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbq30\\AppData\\Local\\Temp\\ipykernel_168572\\3643068280.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y = data.iloc[:, 1].replace({'M': 1, 'B': 0}).values  # Convert labels to binary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('wdbc.data', header=None)\n",
    "X = data.iloc[:, 2:].values\n",
    "y = data.iloc[:, 1].replace({'M': 1, 'B': 0}).values  # Convert labels to binary\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the size of each class in the training set\n",
    "print(\"Training set size:\", np.bincount(y_train))\n",
    "print(\"Validation set size:\", np.bincount(y_val))\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the logistic regression model using mini-batch SGD\n",
    "w, loss_history = mini_batch_sgd(X_train, y_train, batch_size=50, learning_rate=0.01, max_iterations=200)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = sigmoid(np.dot(X_test, w)) >= 0.5\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Summarize findings\n",
    "print(\"Summary of findings will be provided based on the above metrics and any observations during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c22ee3-ec61-44c8-83ca-15f9ffa2d46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
