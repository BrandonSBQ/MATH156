{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c1c5ac-601d-46fb-96ea-cc9768bfbfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 497.9211426654398\n",
      "Iteration 2, Loss: 540.7898916166055\n",
      "Iteration 3, Loss: 1028.5611816142382\n",
      "Iteration 4, Loss: 1550.1783798857768\n",
      "Iteration 5, Loss: 647.5547336556713\n",
      "Iteration 6, Loss: 477.58827761322294\n",
      "Iteration 7, Loss: 691.2441642275571\n",
      "Iteration 8, Loss: 833.135364511259\n",
      "Iteration 9, Loss: 588.5208469146696\n",
      "Iteration 10, Loss: 603.5703227659482\n",
      "Iteration 11, Loss: 503.8168270696686\n",
      "Iteration 12, Loss: 543.1175734550575\n",
      "Iteration 13, Loss: 1096.9677264178504\n",
      "Iteration 14, Loss: 608.4125539486982\n",
      "Iteration 15, Loss: 507.169598381028\n",
      "Iteration 16, Loss: 552.9126682787572\n",
      "Iteration 17, Loss: 544.2837890217443\n",
      "Iteration 18, Loss: 1034.6082851403764\n",
      "Iteration 19, Loss: 558.0777261909301\n",
      "Iteration 20, Loss: 520.7115398236647\n",
      "Iteration 21, Loss: 967.0539294847135\n",
      "Iteration 22, Loss: 729.6271737688521\n",
      "Iteration 23, Loss: 659.1554030363766\n",
      "Iteration 24, Loss: 659.2135307915846\n",
      "Iteration 25, Loss: 550.2468156379222\n",
      "Iteration 26, Loss: 488.8200132502768\n",
      "Iteration 27, Loss: 488.3048143161333\n",
      "Iteration 28, Loss: 510.63425459436473\n",
      "Iteration 29, Loss: 474.4273430245095\n",
      "Iteration 30, Loss: 466.1260755375466\n",
      "Iteration 31, Loss: 470.6123818261381\n",
      "Iteration 32, Loss: 468.39908523897367\n",
      "Iteration 33, Loss: 546.8137155792375\n",
      "Iteration 34, Loss: 604.2803970057472\n",
      "Iteration 35, Loss: 464.2846114526257\n",
      "Iteration 36, Loss: 466.0881736665061\n",
      "Iteration 37, Loss: 478.14612902443133\n",
      "Iteration 38, Loss: 529.8518735978453\n",
      "Iteration 39, Loss: 563.8144561697742\n",
      "Iteration 40, Loss: 968.7481626422357\n",
      "Iteration 41, Loss: 782.0143664164175\n",
      "Iteration 42, Loss: 557.0764899730018\n",
      "Iteration 43, Loss: 667.2908721717903\n",
      "Iteration 44, Loss: 723.0727417466638\n",
      "Iteration 45, Loss: 510.4849854634518\n",
      "Iteration 46, Loss: 533.0176923366591\n",
      "Iteration 47, Loss: 728.2706222259586\n",
      "Iteration 48, Loss: 586.9361595874768\n",
      "Iteration 49, Loss: 463.49920109297045\n",
      "Iteration 50, Loss: 467.68897001868356\n",
      "Iteration 51, Loss: 502.65725436158965\n",
      "Iteration 52, Loss: 445.2299400068116\n",
      "Iteration 53, Loss: 451.90785779103425\n",
      "Iteration 54, Loss: 562.0379235699395\n",
      "Iteration 55, Loss: 685.3290570007721\n",
      "Iteration 56, Loss: 750.1877223234222\n",
      "Iteration 57, Loss: 836.3328043335528\n",
      "Iteration 58, Loss: 836.6842863273091\n",
      "Iteration 59, Loss: 876.9564678345649\n",
      "Iteration 60, Loss: 943.3238564065209\n",
      "Iteration 61, Loss: 1226.632804569961\n",
      "Iteration 62, Loss: 694.0339111653861\n",
      "Iteration 63, Loss: 529.5532237731965\n",
      "Iteration 64, Loss: 521.7603793018653\n",
      "Iteration 65, Loss: 667.9777316826669\n",
      "Iteration 66, Loss: 744.2517274496222\n",
      "Iteration 67, Loss: 560.023777878863\n",
      "Iteration 68, Loss: 511.29740578893575\n",
      "Iteration 69, Loss: 673.4833340013945\n",
      "Iteration 70, Loss: 467.8695127618698\n",
      "Iteration 71, Loss: 478.617296593732\n",
      "Iteration 72, Loss: 946.9533598941596\n",
      "Iteration 73, Loss: 1430.4923704313642\n",
      "Iteration 74, Loss: 673.9619910281742\n",
      "Iteration 75, Loss: 706.4903211078508\n",
      "Iteration 76, Loss: 608.217125707497\n",
      "Iteration 77, Loss: 642.3014629956529\n",
      "Iteration 78, Loss: 537.3041437830877\n",
      "Iteration 79, Loss: 558.6495664241662\n",
      "Iteration 80, Loss: 560.1490695716649\n",
      "Iteration 81, Loss: 598.1295716023509\n",
      "Iteration 82, Loss: 649.4546263957889\n",
      "Iteration 83, Loss: 929.8968173753549\n",
      "Iteration 84, Loss: 567.8259076381571\n",
      "Iteration 85, Loss: 515.5664527016197\n",
      "Iteration 86, Loss: 618.6506562658141\n",
      "Iteration 87, Loss: 664.6691761620805\n",
      "Iteration 88, Loss: 591.4180894507784\n",
      "Iteration 89, Loss: 784.5994741324275\n",
      "Iteration 90, Loss: 1474.087667521629\n",
      "Iteration 91, Loss: 664.9583210709654\n",
      "Iteration 92, Loss: 529.5757189723603\n",
      "Iteration 93, Loss: 587.0105228743208\n",
      "Iteration 94, Loss: 574.2515102092843\n",
      "Iteration 95, Loss: 513.9496561889739\n",
      "Iteration 96, Loss: 604.8067165997808\n",
      "Iteration 97, Loss: 564.6226497177765\n",
      "Iteration 98, Loss: 500.83206324088815\n",
      "Iteration 99, Loss: 516.3237577594072\n",
      "Iteration 100, Loss: 616.0160200638235\n",
      "Final weights: [-0.97558579  0.20809192  0.01417476  0.51267635  0.19228636  0.25631491\n",
      " -0.3889806  -0.37934535  0.68789424 -0.30296638  0.48925349  0.40429837\n",
      " -0.11589479  0.19882636 -0.2440233  -0.04689692  0.25821132 -0.22031326\n",
      " -0.05395518 -0.12267393]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(X, y, w):\n",
    "    \"\"\"Compute the logistic loss function.\"\"\"\n",
    "    z = np.dot(X, w)\n",
    "    y_pred = sigmoid(z)\n",
    "    epsilon = 1e-5  # small number to avoid log(0)\n",
    "    return -np.sum(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
    "\n",
    "def compute_gradient(X, y, w):\n",
    "    \"\"\"Compute the gradient of the logistic loss function.\"\"\"\n",
    "    z = np.dot(X, w)\n",
    "    y_pred = sigmoid(z)\n",
    "    return np.dot(X.T, (y_pred - y))\n",
    "\n",
    "def mini_batch_sgd(X, y, batch_size, learning_rate, max_iterations):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    loss_history = []\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        # Randomly select a batch of samples\n",
    "        indices = np.random.choice(n_samples, batch_size, replace=False)\n",
    "        X_batch = X[indices]\n",
    "        y_batch = y[indices]\n",
    "\n",
    "        # Compute the gradient on the batch\n",
    "        gradient = compute_gradient(X_batch, y_batch, w)\n",
    "        \n",
    "        # Update the weights\n",
    "        w -= learning_rate * gradient\n",
    "        \n",
    "        # Compute and store the loss\n",
    "        loss = compute_loss(X, y, w)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        print(f\"Iteration {iteration + 1}, Loss: {loss}\")\n",
    "\n",
    "    return w, loss_history\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some synthetic data (for demonstration purposes)\n",
    "    from sklearn.datasets import make_classification\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 50\n",
    "    learning_rate = 0.01\n",
    "    max_iterations = 100\n",
    "\n",
    "    # Train the model\n",
    "    w, loss_history = mini_batch_sgd(X, y, batch_size, learning_rate, max_iterations)\n",
    "    print(\"Final weights:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a75a0f-8c37-4321-8541-9ea84c8d193d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
